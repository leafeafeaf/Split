from preprocessing.skeleton import *
import cv2
import tensorflow as tf
import numpy as np
import os
from preprocessing.labeling import labeling
import json
from preprocessing.save_to_Json import save_skel_to_json
import sys
from preprocessing.save_to_Json import save_to_json

# similarity_measure í´ë” ê²½ë¡œë¥¼ ì¶”ê°€
similarity_measure_path = os.path.abspath(os.path.join(os.path.dirname(__file__), "..","..", "similarity_measure"))
sys.path.append(similarity_measure_path)

# ì´ì œ train.pyë¥¼ import ê°€ëŠ¥
# from train import train

def process_videos(input_folder, output_folder, fps=30):
    cnt = 1
    if not os.path.exists(output_folder):
        os.makedirs(output_folder)

    video_files = [f for f in os.listdir(input_folder) if f.endswith('.mp4')]
    video_files.sort(key=lambda f: int(''.join(filter(str.isdigit, f))))
    video_idx = 119

    for video_file in video_files:
        print(video_file)
        # video_path = video_file
        video_path = os.path.join(input_folder, video_file)
        output_path = os.path.join(output_folder, video_file)
        skel_per_video = []
        # ë¹„ë””ì˜¤ ìº¡ì²˜ ê°ì²´ ìƒì„± 
        cap = cv2.VideoCapture(video_path)
        fps = cap.get(cv2.CAP_PROP_FPS)  # ì›ë³¸ ë¹„ë””ì˜¤ FPS ê°€ì ¸ì˜¤ê¸°
        print(f"Original FPS: {fps}")
        # í”„ë ˆì„ì„ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”
        frames = []


        #---------------ìœ ì‚¬ë„ íŒë‹¨ ë°ì´í„° ì €ì¥-------------
        # 'similar' í´ë” ì•ˆì— 'image_data' í´ë” ê²½ë¡œ ì„¤ì •
        output_folder = os.path.join(os.path.dirname(os.path.abspath(__file__)), '..', '..', 'similarity_measure','image_data',f"image_data_{video_idx}")
        output_folder = os.path.abspath(output_folder)
        skel_output_folder = os.path.join(os.path.dirname(os.path.abspath(__file__)), '..', '..', 'similarity_measure','skel_data')
        skel_output_folder = os.path.abspath(skel_output_folder)
        

        # í´ë”ê°€ ì—†ìœ¼ë©´ ìƒì„±
        if not os.path.exists(output_folder):
            os.makedirs(output_folder)
        
        #---------------------------------------------------

        # ë¹„ë””ì˜¤ì˜ ëª¨ë“  í”„ë ˆì„ì„ ì½ê¸°
        frame_idx = 0
        while cap.isOpened():
            ret, frame = cap.read()
            if not ret:
                break
            
            # BGR -> RGB ë³€í™˜ (OpenCVëŠ” ê¸°ë³¸ì ìœ¼ë¡œ BGRë¡œ ì´ë¯¸ì§€ë¥¼ ì½ìŒ)
            frame = cv2.resize(frame, (128, 128))  # í”„ë ˆì„ í¬ê¸° ì¡°ì •
            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            
            
            # í”„ë ˆì„ì„ ë¦¬ìŠ¤íŠ¸ì— ì €ì¥
            frames.append(frame)
            
            frame_idx += 1

        # ë¹„ë””ì˜¤ ìº¡ì²˜ ê°ì²´ í•´ì œ
        cap.release()

        # í”„ë ˆì„ ë¦¬ìŠ¤íŠ¸ê°€ ë¹„ì–´ ìˆëŠ”ì§€ í™•ì¸
        if not frames:
            print(f"No frames collected for {video_file}. Skipping...")
            continue

        # NumPy ë°°ì—´ë¡œ ë³€í™˜
        frames_np = np.array(frames)

        # 4ì°¨ì› ë°°ì—´ì´ ì•„ë‹ˆë©´ ì—ëŸ¬ ë°œìƒ
        if len(frames_np.shape) != 4:
            print(f"Frames array is not 4-dimensional for {video_file}. Skipping...")
            continue

        # TensorFlow í…ì„œë¡œ ë³€í™˜
        image = tf.convert_to_tensor(frames_np, dtype=tf.uint8)
        # í…ì„œ ëª¨ì–‘ í™•ì¸
        num_frames, image_height, image_width, channels = image.shape
        # print(f"Processing video: {video_file}")
        # print(f"Number of frames: {num_frames}, Image height: {image_height}, Image width: {image_width}, Channels: {channels}")

        # Crop ì˜ì—­ ì„¤ì • (ì—¬ê¸°ì„œ ì‚¬ìš©í•˜ëŠ” í•¨ìˆ˜ë“¤ì„ ì •ì˜í•´ë‘ì…”ì•¼ í•©ë‹ˆë‹¤)
        crop_region = init_crop_region(image_height, image_width)

        output_images = []

        # í•œ ë™ì˜ìƒì˜ frameë³„ keypointì¢Œí‘œë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
        
        # bar = display(progress(0, num_frames-1), display_id=True)
        idx = 0
        for frame_idx in range(num_frames):
            keypoint_dataset = [] # keypoint ì¢Œí‘œê°€ ë“¤ì–´ê°€ëŠ” ë°°ì—´
            # print(frame_idx)
            keypoints_with_scores = run_inference(
            movenet, image[frame_idx, :, :, :], crop_region,
            crop_size=[input_size, input_size])

            (keypoint_xy,output_images_input,keypoint_scores) =draw_prediction_on_image(
            image[frame_idx, :, :, :].numpy().astype(np.int32),
            keypoints_with_scores, crop_region=None,
            close_figure=True, output_image_height=300)
        
            print(f"keyopint_input : {output_images_input}")

            # print(f"keypoint_with_scores : {keypoints_with_scores}")
            # print(f"keypoint_with_scores shape : {keypoints_with_scores.shape}")

            # # print(keypoint_xy) # í‚¤ í¬ì¸íŠ¸ ì¶œë ¥ 
            # if keypoint_xy is not None and keypoint_xy.shape[0] == 17:   # 17ê°œ ê´€ì ˆì´ ëª¨ë‘ ìˆì–´ì•¼ í•¨
                
            #     for keypoint_dict_idx in range(len(keypoint_xy)):
            #         for keypoint_xy_idx in range(len(keypoint_xy[0])):
            #             keypoint_dataset.append(keypoint_xy[keypoint_dict_idx][keypoint_xy_idx])
            #             keypoint_dataset.append(keypoints_with_scores)

            # if len(keypoint_dataset) == 34:
            #     # ê° í”„ë ˆì„ì„ ì´ë¯¸ì§€ íŒŒì¼ë¡œ ì €ì¥ (í”„ë ˆì„ ë²ˆí˜¸ë¥¼ íŒŒì¼ëª…ì— ì¶”ê°€) ---- ìœ ì‚¬ë„ íŒë‹¨ë‹¨
            #     frame_filename = os.path.join(output_folder, f'frame_{frame_idx}.jpg')
            #     cv2.imwrite(frame_filename, cv2.cvtColor(frames[frame_idx], cv2.COLOR_RGB2BGR))  # ë‹¤ì‹œ BGRë¡œ ë³€í™˜í•˜ì—¬ ì €ì¥
            #     skel_per_video.append(keypoint_dataset)
            # output_images.append(output_images_input)
            # crop_region = determine_crop_region(
            # keypoints_with_scores, image_height, image_width)
            # print(keypoint_xy) # í‚¤ í¬ì¸íŠ¸ ì¶œë ¥ 
            if keypoint_xy is not None and keypoint_xy.shape[0] == 17:   # 17ê°œ ê´€ì ˆì´ ëª¨ë‘ ìˆì–´ì•¼ í•¨
                idx += 1
                for keypoint_dict_idx in range(len(keypoint_xy)):
                    for keypoint_xy_idx in range(len(keypoint_xy[0])):
                        keypoint_dataset.append(keypoint_xy[keypoint_dict_idx][keypoint_xy_idx])
                    keypoint_dataset.append(keypoint_scores[keypoint_dict_idx])

            # if len(keypoint_dataset) == 34:
                # ê° í”„ë ˆì„ì„ ì´ë¯¸ì§€ íŒŒì¼ë¡œ ì €ì¥ (í”„ë ˆì„ ë²ˆí˜¸ë¥¼ íŒŒì¼ëª…ì— ì¶”ê°€) ---- ìœ ì‚¬ë„ íŒë‹¨ë‹¨
                frame_filename = os.path.join(output_folder, f'frame_{idx}.jpg')
                cv2.imwrite(frame_filename, cv2.cvtColor(output_images_input, cv2.COLOR_RGB2BGR))  # ë‹¤ì‹œ BGRë¡œ ë³€í™˜í•˜ì—¬ ì €ì¥
                # cv2.imwrite(frame_filename, cv2.cvtColor(frames[frame_idx], cv2.COLOR_RGB2BGR))  # ë‹¤ì‹œ BGRë¡œ ë³€í™˜í•˜ì—¬ ì €ì¥
                skel_per_video.append(keypoint_dataset)
            output_images.append(output_images_input)
            crop_region = determine_crop_region(
            keypoints_with_scores, image_height, image_width)
            # bar.update(progress(frame_idx, num_frames-1))
        # output_images.append(draw_prediction_on_image(
        #       image[frame_idx, :, :, :].numpy().astype(np.int32),
        #       keypoints_with_scores, crop_region=None,
        #       close_figure=True, output_image_height=300))
        output = np.stack(output_images, axis=0)
        print(f"skel_per_video : {skel_per_video}")
        print(f"skel_per_video_len : {len(skel_per_video)}")

        skel_output = {
            'keypoints': skel_per_video
        }

        # ìŠ¤ì¼ˆë ˆí†¤ ë°ì´í„° ì €ì¥ ----- ìœ ì‚¬ë„ íŒë‹¨ë‹¨
        save_skel_to_json(skel_output,skel_output_folder,video_idx)
        video_idx +=1

    skel_filename = os.path.join(skel_output_folder, f'skel_data_{video_idx:03d}.json')


    # train(output_folder,skel_filename)
        
        # labeling(video_file,skel_per_video)

        # skel_dataset.append(skel_per_video)

    # return skel_dataset



def interpolate_keypoints_between_frames(prev_keypoints, next_keypoints, gap, num_keypoints=17):
    """ë‘ í”„ë ˆì„ ì‚¬ì´ì˜ í‚¤í¬ì¸íŠ¸ë¥¼ ë¹„ë¡€ì ìœ¼ë¡œ ë³´ê°„í•©ë‹ˆë‹¤."""
    keypoints_interpolated = []
    
    for i in range(num_keypoints):
        prev_keypoint = prev_keypoints[i]
        next_keypoint = next_keypoints[i]
        
        # X, Y ì¢Œí‘œì™€ ìŠ¤ì½”ì–´ë¥¼ ë¹„ë¡€ì ìœ¼ë¡œ ë³´ê°„
        interpolated_keypoint = prev_keypoint + (next_keypoint - prev_keypoint) * np.linspace(0, 1, gap)
        keypoints_interpolated.append(interpolated_keypoint)
        
    return np.array(keypoints_interpolated)

def process_video_infer(video_path, output_folder, fps=30):
    # âœ… output_folder ë§¤ê°œë³€ìˆ˜ ì¶”ê°€
    if not os.path.exists(output_folder):
        os.makedirs(output_folder)

    # ë¹„ë””ì˜¤ ìº¡ì²˜ ê°ì²´ ìƒì„±
    cap = cv2.VideoCapture(video_path)
    original_fps = cap.get(cv2.CAP_PROP_FPS)  # ì›ë³¸ ë¹„ë””ì˜¤ FPS ê°€ì ¸ì˜¤ê¸°
    print(f"Original FPS: {original_fps}")

    # í”„ë ˆì„ì„ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”
    frames = []
    frames_128 = []

    # ë¹„ë””ì˜¤ì˜ ëª¨ë“  í”„ë ˆì„ì„ ì½ê¸°
    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break

        frame_128 = cv2.resize(frame, (128, 128))  # í”„ë ˆì„ í¬ê¸° ì¡°ì •
        frame = cv2.resize(frame, (640, 640))  # í”„ë ˆì„ í¬ê¸° ì¡°ì •

        # BGR -> RGB ë³€í™˜ (OpenCVëŠ” ê¸°ë³¸ì ìœ¼ë¡œ BGRë¡œ ì´ë¯¸ì§€ë¥¼ ì½ìŒ)
        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        frame_128 = cv2.cvtColor(frame_128, cv2.COLOR_BGR2RGB)

        frames.append(frame)
        frames_128.append(frame_128)

    # ë¹„ë””ì˜¤ ìº¡ì²˜ ê°ì²´ í•´ì œ
    cap.release()

    # NumPy ë°°ì—´ë¡œ ë³€í™˜
    frames_np = np.array(frames)

    # âœ… í”„ë ˆì„ì´ ë¹„ì–´ìˆëŠ” ê²½ìš° ì˜ˆì™¸ ì²˜ë¦¬ ì¶”ê°€
    if frames_np.size == 0:
        print(f"No frames found in {video_path}. Skipping...")
        return None, None

    # TensorFlow í…ì„œë¡œ ë³€í™˜
    image = tf.convert_to_tensor(frames_np, dtype=tf.uint8)

    # í…ì„œ ëª¨ì–‘ í™•ì¸
    num_frames, image_height, image_width, channels = image.shape
    print(f"Processing video: {video_path}")
    print(f"Number of frames: {num_frames}, Image height: {image_height}, Image width: {image_width}, Channels: {channels}")

    # Crop ì˜ì—­ ì„¤ì •
    crop_region = init_crop_region(image_height, image_width)

    output_images = []
    skel_datasetof2 = []
    skel_dataset = []

    # í‚¤í¬ì¸íŠ¸ê°€ 17ê°œì¸ í”„ë ˆì„ ì°¾ê¸°
    keypoints_with_scores_per_frame = []

    for frame_idx in range(num_frames):
        keypoint_datasetof2 = []
        keypoint_dataset = []

        # í‚¤í¬ì¸íŠ¸ ì¶”ë¡  ìˆ˜í–‰
        keypoints_with_scores = run_inference(
            movenet, image[frame_idx, :, :, :], crop_region,
            crop_size=[input_size, input_size]
        )

        # í‚¤í¬ì¸íŠ¸ ì‹œê°í™” ë° ë°ì´í„° ì¶”ì¶œ
        keypoint_xy, output_image, keypoint_scores = draw_prediction_on_image(
            image[frame_idx, :, :, :].numpy().astype(np.int32),
            keypoints_with_scores, crop_region=None,
            close_figure=True, output_image_height=300
        )

        # í‚¤í¬ì¸íŠ¸ ë°ì´í„° ì €ì¥
        if keypoint_xy is not None and keypoint_xy.shape[0] == 17:
            print(f"frame_idx = {frame_idx}")
            for keypoint_dict_idx in range(len(keypoint_xy)):
                if keypoint_xy.ndim > 1:  # keypoint_xyê°€ 2D ë°°ì—´ì¼ ë•Œ
                    keypoint_dataset.append(keypoint_xy[keypoint_dict_idx][0])  # X ì¢Œí‘œ
                    keypoint_dataset.append(keypoint_xy[keypoint_dict_idx][1])  # Y ì¢Œí‘œ
                else:  # keypoint_xyê°€ 1D ë°°ì—´ì¼ ê²½ìš°
                    keypoint_dataset.append(keypoint_xy[keypoint_dict_idx])  # ë‹¨ì¼ ì¢Œí‘œ

                keypoint_dataset.append(keypoint_scores[keypoint_dict_idx])  # ìŠ¤ì½”ì–´ ì¶”ê°€

                if keypoint_xy.ndim > 1:
                    keypoint_datasetof2.append(keypoint_xy[keypoint_dict_idx][0])
                    keypoint_datasetof2.append(keypoint_xy[keypoint_dict_idx][1])
                else:
                    keypoint_datasetof2.append(keypoint_xy[keypoint_dict_idx])

            skel_dataset.append(keypoint_dataset)  # keypoint_dataset : [51ê°œ]
            skel_datasetof2.append(keypoint_datasetof2)
        else:
            print(f"frame_idx = {frame_idx}ì—ì„œ í‚¤í¬ì¸íŠ¸ ë°ì´í„°ê°€ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")

        output_images.append(output_image)

        # ìƒˆë¡œìš´ crop_region ì—…ë°ì´íŠ¸
        crop_region = determine_crop_region(
            keypoints_with_scores, image_height, image_width
        )

    # ë³´ê°„ ì²˜ë¦¬: ë‘ 17ê°œ í‚¤í¬ì¸íŠ¸ê°€ ìˆëŠ” í”„ë ˆì„ ì‚¬ì´ì˜ í”„ë ˆì„ì„ ë³´ê°„
    interpolated_keypoints = []
    for i in range(1, len(keypoints_with_scores_per_frame)):
        prev_keypoints, prev_scores = keypoints_with_scores_per_frame[i - 1]
        next_keypoints, next_scores = keypoints_with_scores_per_frame[i]

        if prev_keypoints is not None and next_keypoints is not None:
            # ë‘ 17ê°œ í‚¤í¬ì¸íŠ¸ê°€ ìˆëŠ” í”„ë ˆì„ ì‚¬ì´ì˜ ê°„ê²© ê³„ì‚°
            gap = i - (i - 1)  # ë‘ í”„ë ˆì„ ì‚¬ì´ì˜ ê°„ê²©
            interpolated = interpolate_keypoints_between_frames(prev_keypoints, next_keypoints, gap)
            interpolated_keypoints.append(interpolated)

    # ìµœì¢… í‚¤í¬ì¸íŠ¸ ë°ì´í„° ì²˜ë¦¬
    for keypoint_xy, keypoint_scores in keypoints_with_scores_per_frame:
        if keypoint_xy is None:  # ë³´ê°„ëœ í‚¤í¬ì¸íŠ¸ê°€ ìˆì„ ê²½ìš°
            if interpolated_keypoints:  # ë³´ê°„ëœ í‚¤í¬ì¸íŠ¸ê°€ ë‚¨ì•„ ìˆë‹¤ë©´
                interpolated_data = interpolated_keypoints.pop(0)
                keypoint_xy = interpolated_data[0]  # ì²« ë²ˆì§¸ ê°’ì€ keypoint_xy
                keypoint_scores = interpolated_data[1]  # ë‘ ë²ˆì§¸ ê°’ì€ keypoint_scores
        else:
            keypoint_xy, keypoint_scores = None, None

        if keypoint_xy is not None:
            for keypoint_dict_idx in range(len(keypoint_xy)):
                keypoint_dataset.append(keypoint_xy[keypoint_dict_idx][0])
                keypoint_dataset.append(keypoint_xy[keypoint_dict_idx][1])
                keypoint_dataset.append(keypoint_scores[keypoint_dict_idx])

                keypoint_datasetof2.append(keypoint_xy[keypoint_dict_idx][0])
                keypoint_datasetof2.append(keypoint_xy[keypoint_dict_idx][1])

            skel_dataset.append(keypoint_dataset)
            skel_datasetof2.append(keypoint_datasetof2)

    return skel_dataset, skel_datasetof2, output_images, num_frames


def release_capture(input_folder, output_folder, fps=30):
    if not os.path.exists(output_folder):
        os.makedirs(output_folder)

    video_files = [f for f in os.listdir(input_folder) if f.endswith('.mp4')]
    video_files.sort(key=lambda f: int(''.join(filter(str.isdigit, f))))  # ìˆ«ìë¡œ ì •ë ¬
    print(f"í˜„ì¬ í´ë”ì˜ ë¹„ë””ì˜¤ ê°œìˆ˜: {len(video_files)}")
    video_idx = 52
    for video_file in video_files:
        print(f"Processing {video_file}...")
        video_path = os.path.join(input_folder, video_file)

        cap = cv2.VideoCapture(video_path)
        fps = cap.get(cv2.CAP_PROP_FPS)  # FPS ê°€ì ¸ì˜¤ê¸°
        print(f"Original FPS: {fps}")

        max_distance = -10000
        max_distance_frame = None
        max_distance_keypoints = None
        
        frame_idx = 0
        avg_distance_last_4 = 0
        frames = []
        distance_list = []
        while cap.isOpened():
            ret, frame = cap.read()
            if not ret:
                break

            # frame = cv2.resize(frame, (640, 640))  # í”„ë ˆì„ í¬ê¸° ì¡°ì •
            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)  # RGB ë³€í™˜

            # TensorFlow í…ì„œ ë³€í™˜
            image = tf.convert_to_tensor(frame, dtype=tf.uint8)
            image_height, image_width, channels = image.shape
            crop_region = init_crop_region(image_height, image_width)
            keypoints_with_scores = run_inference(movenet, image, crop_region, crop_size=[input_size, input_size])

            # Keypoint ë°ì´í„° ì²˜ë¦¬
            keypoint_xy, _ = draw_prediction_on_image(
                image.numpy().astype(np.int32),
                keypoints_with_scores, crop_region=None,
                close_figure=True, output_image_height=300
            )
            print(keypoint_xy.shape)
            # ğŸ”¥ 17ê°œì˜ keypointê°€ ìˆëŠ” ê²½ìš°ë§Œ ì²˜ë¦¬
            if keypoint_xy is not None and keypoint_xy.shape[0] == 17:
                # ğŸ”¥ ì™¼ìª½/ì˜¤ë¥¸ìª½ ë°œëª© ìœ„ì¹˜
                
                left_foot = np.array(keypoint_xy[16][0])
                right_foot = np.array(keypoint_xy[15][0])
                foot = np.array([left_foot, right_foot])
                min_foot = np.minimum(left_foot, right_foot)
                shoulder =  np.array([keypoint_xy[5][0], keypoint_xy[6][0]])
                shoulder_avg = np.average(shoulder)
                ankle_left = np.array([keypoint_xy[15][0], keypoint_xy[15][1]])  # Keypoint 15 (ì™¼ìª½ ë°œëª©)
                ankle_right = np.array([keypoint_xy[16][0], keypoint_xy[16][1]])  # Keypoint 16 (ì˜¤ë¥¸ìª½ ë°œëª©)
                # distance = np.linalg.norm(ankle_left - ankle_right)  # ë‘ ë°œëª© ê±°ë¦¬ ê³„ì‚°
                distance = abs(left_foot - right_foot)
                distance_list.append(distance)
                if len(distance_list) > 6: distance_list.pop(0)  # ê°€ì¥ ì˜¤ë˜ëœ ê°’ ì œê±°

                # ğŸ”¥ 4ê°œ ì´ìƒ ìŒ“ì˜€ì„ ë•Œ í‰ê·  ê³„ì‚°
                if len(distance_list) == 6: avg_distance_last_4 = np.mean(distance_list)
                print(f"{frame_idx} ë²ˆì§¸ í”„ë ˆì„ì—ì„œ ë°œ ê°„ê²© í‰ê·  : {avg_distance_last_4}")

                cv2.putText(frame, str(f"{frame_idx} : {distance}"), (0, 50), cv2.FONT_HERSHEY_COMPLEX, 0.7, (0, 255, 255), 2)
                frames.append(frame)

                # ğŸ”¥ í˜„ì¬ í”„ë ˆì„ì´ ê°€ì¥ í° ê±°ë¦¬ë¼ë©´ ê°±ì‹ 
                if avg_distance_last_4 > max_distance:
                    max_distance = avg_distance_last_4
                    max_distance_frame = frame  # ê°€ì¥ ë„“ì€ í”„ë ˆì„ ì €ì¥
                    max_distance_keypoints = keypoint_xy #[coord for keypoint in keypoint_xy for coord in keypoint]  # 34ê°œ ê°’ ì €ì¥

            frame_idx += 1

        cap.release()
        # ğŸ”¥ ìŠ¤ì¼ˆë ˆí†¤ ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ìŠ¤í‚µ
        if max_distance_frame is None or max_distance_keypoints is None:
            print(f"No valid keypoint data for {video_file}. Skipping...")
            continue

        print(f"ê°€ì¥ ë„“ì€ ë°œ ê°„ê²© í”„ë ˆì„: {max_distance}")

        # âœ… **ì´ë¯¸ì§€ ì €ì¥**
        release_image_folder = os.path.join(output_folder, "release_images")
        os.makedirs(release_image_folder, exist_ok=True)
        # to_mp4(frames, fps=30, input_file_path="C:\\Users\\SSAFY\\code\\PJT\\S12P11B202\\AI\\posture_correct\\release_images\\test.mp4", output_folder=output_folder)
        frame_bgr = cv2.cvtColor(max_distance_frame, cv2.COLOR_RGB2BGR)  # ë‹¤ì‹œ BGR ë³€í™˜ (OpenCV ì €ì¥ìš©)
        frame_path = os.path.join(release_image_folder, f"video{video_idx}.jpg")
        cv2.imwrite(frame_path, frame_bgr)  # ì´ë¯¸ì§€ ì €ì¥
        print(f"âœ… ì €ì¥ëœ ì´ë¯¸ì§€: {frame_path}")

        # âœ… **JSON ì €ì¥**
        dataset = []
        dataset.append(max_distance_keypoints)
        save_to_json(np.array(dataset))
        video_idx += 1
    return max_distance_keypoints




def release_capture_for_infer(skel_list, fps=30):
        
        max_distance = -10000
        max_distance_keypoints = None

        frame_idx = 0
        avg_distance_last_4 = 0
        distance_list = []
        for skel in skel_list:
                frame_idx += 1
                # ğŸ”¥ ì™¼ìª½/ì˜¤ë¥¸ìª½ ë°œëª© ìœ„ì¹˜
                left_foot = np.array(skel[32])
                right_foot = np.array(skel[30])
                distance = abs(left_foot - right_foot)
                distance_list.append(distance)
                if len(distance_list) > 5: distance_list.pop(0)  # ê°€ì¥ ì˜¤ë˜ëœ ê°’ ì œê±°

                # ğŸ”¥ 4ê°œ ì´ìƒ ìŒ“ì˜€ì„ ë•Œ í‰ê·  ê³„ì‚°
                if len(distance_list) == 5: avg_distance_last_4 = np.mean(distance_list)

                print(f"{frame_idx} ë²ˆì§¸ í”„ë ˆì„ì—ì„œ ë°œ ê±°ë¦¬: {distance}")

                # ğŸ”¥ í˜„ì¬ í”„ë ˆì„ì´ ê°€ì¥ í° ê±°ë¦¬ë¼ë©´ ê°±ì‹ 
                if avg_distance_last_4 > max_distance:
                    max_distance = avg_distance_last_4
                    max_distance_keypoints = skel #[coord for keypoint in keypoint_xy for coord in keypoint]  # 34ê°œ ê°’ ì €ì¥
            
        
        return max_distance_keypoints

def to_mp4(images, fps, input_file_path, output_folder):
    print("IN to_mp4 --- ")
    print(images[0].shape)
    height, width, _ = images[0].shape
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # mp4v ì½”ë± ì‚¬ìš©
    input_file_name = os.path.basename(input_file_path)  # ì…ë ¥ íŒŒì¼ ì´ë¦„ (ê²½ë¡œ ì œì™¸)
    output_file = os.path.join(output_folder, input_file_name)  # ì¶œë ¥ í´ë”ì— ë™ì¼í•œ íŒŒì¼ ì´ë¦„ ì‚¬ìš©
    video_writer = cv2.VideoWriter(output_file, fourcc, fps, (width, height))

    for image in images:
        # ì´ë¯¸ì§€ ë°ì´í„°ë¥¼ BGRë¡œ ë³€í™˜ (OpenCVëŠ” BGR í¬ë§·ì„ ì‚¬ìš©)
        bgr_image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)
        video_writer.write(bgr_image)

    video_writer.release()
    print(f"Video saved as {output_file}")